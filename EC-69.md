This project demonstrates how to query streaming data from Kafka using several Azure technologies:

Kafka Mirror Maker
Azure Event Hubs
Azure Stream Analytics
Azure Logic Apps
Service Now Integration
Workflow:

Generator App sents message to Kafka or Event Hubs
Stream Analytics will aggregate and filter messages into a second Event Hubs
Logic App will read events and create a ticket in Service Now





Today EC is reading from Kafka, but the reporting is all done via Azure Event Hub. This all needs to change to a new pattern that is not tightly coupled to Azure services, while also being mindful of the value this data has within the system. IT Ops needs to see these device events to ensure that work is being done before a device failure. ODS also need to make use of this data for reporting. 

MVP:

Replicate the logic that occurs in Event Hub into an AWS pattern to land SQL based insights into Service Now
Replace Apache NiFi to land device events/heartbeat data in current state as-is to ODS staging tables
Post go-live: Need to figure out what we're doing long term with the functionality that Solar Winds provides today



[OnPrem Kafka / Generator App] -> [MirrorMaker / MSK Replicator] -> [Amazon MSK (telemetry.raw, heartbeat.raw, alerts.raw)]
[Amazon MSK] -> [MSK Connect -> JDBC Sink -> ODS Staging (RDS/Aurora)]   // replaces NiFi (MVP)
[Amazon MSK] -> [Managed Flink (Flink SQL jobs)] -> [alerts.derived / telemetry.derived topics]
[alerts.derived] -> [Lambda (MSK consumer)] -> [ServiceNow REST API]
[Amazon MSK] -> [MSK Connect -> S3 raw lake]
[Observability] <- CloudWatch metrics, Prometheus (JMX exporter), Grafana
(Security): VPC-only access, TLS, SASL/IAM, KMS for at-rest encryption



Flow 1 — Replace Azure Stream path (paste into Lucidchart text-to-diagram)

(One-line: On-prem Kafka → replicate → MSK → Flink SQL → derived topics → Lambda → ServiceNow)

[Vending Machine / Generator App] -> [Event Collector (Edge Gateway)]
[Event Collector (Edge Gateway)] -> [On-Prem Kafka]
[On-Prem Kafka] -> [MirrorMaker / Replicator] -> [Amazon MSK (telemetry.raw, heartbeat.raw, alerts.raw)]
[Amazon MSK] -> [Managed Flink (Flink SQL Jobs) | consumes telemetry.raw, alerts.raw]
[Managed Flink (Flink SQL Jobs)] -> [alerts.derived, telemetry.derived topics (on MSK)]
[alerts.derived] -> [Lambda (MSK consumer / transformer)]
[Lambda (MSK consumer / transformer)] -> [ServiceNow REST API (create incident)]
[Amazon MSK] -> [MSK Connect -> S3 (raw archive)]
[Observability] <- CloudWatch Metrics, Prometheus (JMX exporter), Grafana
[Alerting] <- CloudWatch Alarms -> SNS Topic -> Lambda -> ServiceNow (Ops alerts)
(Security): VPC, TLS, SASL/IAM, KMS
(Notes): "Use partition by device-id/site-id for ordering; Flink handles ASA-like SQL transforms & windows"


Flow 2 — Replace NiFi landing to ODS staging (paste into Lucidchart text-to-diagram)

(One-line: MSK Connect JDBC Sink writes raw device/heartbeat events directly into ODS staging tables; S3 for archival)

[Device / Event Collector] -> [On-Prem Kafka]
[On-Prem Kafka] -> [MirrorMaker / Replicator] -> [Amazon MSK (telemetry.raw, heartbeat.raw)]
[Amazon MSK] -> [MSK Connect -> JDBC Sink -> ODS Staging (RDS / Aurora / External JDBC target)]
[Amazon MSK] -> [MSK Connect -> S3 (raw partitioned archive by date/device)]
[Optional] [Amazon MSK] -> [Consumer App / Batch ETL] -> [Transform -> Final ODS / Data Warehouse]
[Monitoring] <- CloudWatch Logs + Connector Metrics
[DLQ / Errors] -> [SQS or SNS] -> [Ops Lambda / Manual Review Workflow]
(Security): VPC, TLS, IAM, Secrets Manager for DB creds, KMS
(Notes): "MVP replaces NiFi with managed JDBC sink; retain NiFi in read-only mode for complex provenance if needed."

Compact Flow
-----------------------

External:
  Event Generator (Device) -> Kafka (Topics)
  Event Generator (Device) -> Azure Event Hubs (direct ingest)

Ingest:
  Kafka (Topics) -> Kafka MirrorMaker -> Event Hubs Namespace

Transform:
  Event Hubs Namespace -> Event Hub (aggregated stream)
  Event Hub -> Azure Stream Analytics -> Event Hub (filtered/output)

Notification:
  Event Hub -> Azure Logic Apps -> ServiceNow (create ticket)

Detailed Swimlanes
------------------------
Swimlane: External
  [Event Generator - Vending Machine] --AMQP / HTTPS / Kafka--> [Event Collector / Generator App]
  [Event Generator - Vending Machine] --kafka://tcp--> [Kafka Topics]

Swimlane: Ingest
  [Kafka Topics] --> [Kafka MirrorMaker]
  [Kafka MirrorMaker] --> [Event Hubs Namespace]
  [Event Generator] --(optional direct)--> [Azure Event Hubs] 

Swimlane: Transform
  [Event Hubs Namespace] --> [Event Hub 1]
  [Event Hubs Namespace] --> [Event Hub 2]
  [Event Hub N] --> [Azure Stream Analytics]
  [Azure Stream Analytics] --> [Event Hub (aggregated/filtered output)]

Swimlane: Notification
  [Event Hub (aggregated/filtered output)] --> [Azure Logic Apps]
  [Azure Logic Apps] --> [ServiceNow (now) : Create incident/ticket]

Notes:
  - Kafka holds multiple topics (Topic1, Topic2,... TopicN)
  - Stream Analytics aggregates & filters messages before output
  - Logic App reads from Event Hub and triggers ServiceNow integration



### Option -A
Event Generator -> Kafka
Kafka -> Kafka MirrorMaker
Kafka MirrorMaker -> Azure Event Hubs Namespace
Azure Event Hubs Namespace -> Event Hub (Raw Stream)
Event Hub (Raw Stream) -> Azure Stream Analytics
Azure Stream Analytics -> Event Hub (Filtered Output)
Event Hub (Filtered Output) -> Azure Logic Apps
Azure Logic Apps -> ServiceNow (Ticket Creation)


### option-B
[Event Generator] --> [Kafka]
[Event Generator] --> [Azure Event Hubs (direct)]
[Kafka] --> [Kafka MirrorMaker]
[Kafka MirrorMaker] --> [Event Hubs Namespace]
[Event Hubs Namespace] --> [Event Hub]
[Event Hub] --> [Azure Stream Analytics]
[Azure Stream Analytics] --> [Event Hub (Output)]
[Event Hub (Output)] --> [Azure Logic Apps]
[Azure Logic Apps] --> [ServiceNow]


### Option-C
flowchart LR
    EG(Event Generator) --> K(Kafka)
    EG --> EHDirect(Azure Event Hubs - Direct Ingest)
    K --> MM(Kafka MirrorMaker)
    MM --> EHNS(Event Hubs Namespace)
    EHNS --> EHRaw(Event Hub - Raw Messages)
    EHRaw --> ASA(Azure Stream Analytics)
    ASA --> EHOut(Event Hub - Filtered Output)
    EHOut --> LA(Azure Logic Apps)
    LA --> SN(ServiceNow)



### Redshift

Short answer — **keep the MSK + Flink (stream processing) + MSK Connect → ODS pattern**, and **use Amazon Redshift as the analytics/BI sink**, **not** as a replacement for the stream processor or for the immediate ticketing/alerting path.

Why — the single crisp reasons:

* **Redshift is a data warehouse (SQL analytics, historical reporting, BI).** It’s excellent for storing aggregated/curated results and running heavy SQL reporting, dashboards, ad-hoc queries. ([Amazon Web Services, Inc.][1])
* **Redshift is not a stream-processing engine.** It doesn’t replace Flink/Apache Kafka for windowing, exactly-once stateful transformations, low-latency event detection, or driving ServiceNow incident logic in real time. For those tasks you need a stream processor (Flink/Kafka Streams) or lightweight consumers (Lambda). ([Amazon Web Services, Inc.][2])
* **Good news:** Redshift *can* ingest streaming data (direct streaming ingestion from MSK/Kinesis into materialized views), so it can receive the derived outputs of your stream jobs for near-real-time analytics and reporting — but it’s a *sink*, not a replacement for the stream compute. Use it downstream. ([AWS Documentation][3])

Concrete placement in your architecture (one clear sentence each):

* **Real-time detection & ticketing (ServiceNow):** Keep **MSK → Flink (Managed) → derived topic(s) → Lambda → ServiceNow**. Flink handles the SQL/windowing/alerts reliably and with low latency. ([Amazon Web Services, Inc.][2])
* **ODS staging (NiFi replacement MVP):** Keep **MSK Connect → JDBC sink → ODS (RDS/Aurora)** to land events as-is. This preserves the “as-is” landing requirement. ([AWS Documentation][4])
* **Analytics & reporting (where Redshift helps):** Stream the *derived* topic(s) or use MSK Connect / Firehose / Redshift streaming ingestion to populate Redshift materialized views / tables for BI, dashboards and historical SQL queries. This gives you SQL-based insights and reporting without using Azure services. ([AWS Documentation][3])

When to *use* Redshift (quick checklist):

* You need **fast, complex SQL analytics**, multi-TB scans, BI dashboards (Looker/QuickSight/Tableau). ([Amazon Web Services, Inc.][1])
* You want a **single place for curated, historical, aggregated data**, e.g., daily/weekly device health reports, SLA dashboards, long-term trend analysis.
* You want to **avoid staging everything in S3 + EMR** for ad-hoc SQL — Redshift can host curated tables and support concurrent BI users.

When *not* to use Redshift:

* For **low-latency alerting**, **windowed event detection**, **exactly-once stateful processing** (use Flink/MSK). ([Amazon Web Services, Inc.][2])
* As a direct substitute for NiFi’s provenance/flow features — use MSK Connect or keep NiFi for complex flows.

Cost/operational note (brief):

* Redshift is optimized for analytical queries and storage; streaming ingestion into Redshift is supported and can be efficient, but heavy, high-cardinality streaming workloads are typically cheaper to archive in S3 and process with streaming engines + periodic loads into Redshift. Balance retention, query patterns and cost. ([Amazon Web Services, Inc.][1])

Final one-line recommendation (the decisive answer):
**Do not replace Flink/MSK with Redshift.** Keep **MSK + Flink + MSK Connect → ODS** for the MVP and add **Redshift downstream** as your analytics/SQL reporting store (fed from derived topics or via streaming ingestion) to satisfy the SQL-insight/reporting needs and BI users. ([AWS Documentation][3])


