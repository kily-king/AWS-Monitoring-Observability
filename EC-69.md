This project demonstrates how to query streaming data from Kafka using several Azure technologies:

Kafka Mirror Maker
Azure Event Hubs
Azure Stream Analytics
Azure Logic Apps
Service Now Integration
Workflow:

Generator App sents message to Kafka or Event Hubs
Stream Analytics will aggregate and filter messages into a second Event Hubs
Logic App will read events and create a ticket in Service Now





Today EC is reading from Kafka, but the reporting is all done via Azure Event Hub. This all needs to change to a new pattern that is not tightly coupled to Azure services, while also being mindful of the value this data has within the system. IT Ops needs to see these device events to ensure that work is being done before a device failure. ODS also need to make use of this data for reporting. 

MVP:

Replicate the logic that occurs in Event Hub into an AWS pattern to land SQL based insights into Service Now
Replace Apache NiFi to land device events/heartbeat data in current state as-is to ODS staging tables
Post go-live: Need to figure out what we're doing long term with the functionality that Solar Winds provides today



[OnPrem Kafka / Generator App] -> [MirrorMaker / MSK Replicator] -> [Amazon MSK (telemetry.raw, heartbeat.raw, alerts.raw)]
[Amazon MSK] -> [MSK Connect -> JDBC Sink -> ODS Staging (RDS/Aurora)]   // replaces NiFi (MVP)
[Amazon MSK] -> [Managed Flink (Flink SQL jobs)] -> [alerts.derived / telemetry.derived topics]
[alerts.derived] -> [Lambda (MSK consumer)] -> [ServiceNow REST API]
[Amazon MSK] -> [MSK Connect -> S3 raw lake]
[Observability] <- CloudWatch metrics, Prometheus (JMX exporter), Grafana
(Security): VPC-only access, TLS, SASL/IAM, KMS for at-rest encryption



Flow 1 — Replace Azure Stream path (paste into Lucidchart text-to-diagram)

(One-line: On-prem Kafka → replicate → MSK → Flink SQL → derived topics → Lambda → ServiceNow)

[Vending Machine / Generator App] -> [Event Collector (Edge Gateway)]
[Event Collector (Edge Gateway)] -> [On-Prem Kafka]
[On-Prem Kafka] -> [MirrorMaker / Replicator] -> [Amazon MSK (telemetry.raw, heartbeat.raw, alerts.raw)]
[Amazon MSK] -> [Managed Flink (Flink SQL Jobs) | consumes telemetry.raw, alerts.raw]
[Managed Flink (Flink SQL Jobs)] -> [alerts.derived, telemetry.derived topics (on MSK)]
[alerts.derived] -> [Lambda (MSK consumer / transformer)]
[Lambda (MSK consumer / transformer)] -> [ServiceNow REST API (create incident)]
[Amazon MSK] -> [MSK Connect -> S3 (raw archive)]
[Observability] <- CloudWatch Metrics, Prometheus (JMX exporter), Grafana
[Alerting] <- CloudWatch Alarms -> SNS Topic -> Lambda -> ServiceNow (Ops alerts)
(Security): VPC, TLS, SASL/IAM, KMS
(Notes): "Use partition by device-id/site-id for ordering; Flink handles ASA-like SQL transforms & windows"


Flow 2 — Replace NiFi landing to ODS staging (paste into Lucidchart text-to-diagram)

(One-line: MSK Connect JDBC Sink writes raw device/heartbeat events directly into ODS staging tables; S3 for archival)

[Device / Event Collector] -> [On-Prem Kafka]
[On-Prem Kafka] -> [MirrorMaker / Replicator] -> [Amazon MSK (telemetry.raw, heartbeat.raw)]
[Amazon MSK] -> [MSK Connect -> JDBC Sink -> ODS Staging (RDS / Aurora / External JDBC target)]
[Amazon MSK] -> [MSK Connect -> S3 (raw partitioned archive by date/device)]
[Optional] [Amazon MSK] -> [Consumer App / Batch ETL] -> [Transform -> Final ODS / Data Warehouse]
[Monitoring] <- CloudWatch Logs + Connector Metrics
[DLQ / Errors] -> [SQS or SNS] -> [Ops Lambda / Manual Review Workflow]
(Security): VPC, TLS, IAM, Secrets Manager for DB creds, KMS
(Notes): "MVP replaces NiFi with managed JDBC sink; retain NiFi in read-only mode for complex provenance if needed."
