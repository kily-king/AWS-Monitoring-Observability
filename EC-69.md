This project demonstrates how to query streaming data from Kafka using several Azure technologies:

Kafka Mirror Maker
Azure Event Hubs
Azure Stream Analytics
Azure Logic Apps
Service Now Integration
Workflow:

Generator App sents message to Kafka or Event Hubs
Stream Analytics will aggregate and filter messages into a second Event Hubs
Logic App will read events and create a ticket in Service Now





Today EC is reading from Kafka, but the reporting is all done via Azure Event Hub. This all needs to change to a new pattern that is not tightly coupled to Azure services, while also being mindful of the value this data has within the system. IT Ops needs to see these device events to ensure that work is being done before a device failure. ODS also need to make use of this data for reporting. 

MVP:

Replicate the logic that occurs in Event Hub into an AWS pattern to land SQL based insights into Service Now
Replace Apache NiFi to land device events/heartbeat data in current state as-is to ODS staging tables
Post go-live: Need to figure out what we're doing long term with the functionality that Solar Winds provides today



[OnPrem Kafka / Generator App] -> [MirrorMaker / MSK Replicator] -> [Amazon MSK (telemetry.raw, heartbeat.raw, alerts.raw)]
[Amazon MSK] -> [MSK Connect -> JDBC Sink -> ODS Staging (RDS/Aurora)]   // replaces NiFi (MVP)
[Amazon MSK] -> [Managed Flink (Flink SQL jobs)] -> [alerts.derived / telemetry.derived topics]
[alerts.derived] -> [Lambda (MSK consumer)] -> [ServiceNow REST API]
[Amazon MSK] -> [MSK Connect -> S3 raw lake]
[Observability] <- CloudWatch metrics, Prometheus (JMX exporter), Grafana
(Security): VPC-only access, TLS, SASL/IAM, KMS for at-rest encryption



Flow 1 — Replace Azure Stream path (paste into Lucidchart text-to-diagram)

(One-line: On-prem Kafka → replicate → MSK → Flink SQL → derived topics → Lambda → ServiceNow)

[Vending Machine / Generator App] -> [Event Collector (Edge Gateway)]
[Event Collector (Edge Gateway)] -> [On-Prem Kafka]
[On-Prem Kafka] -> [MirrorMaker / Replicator] -> [Amazon MSK (telemetry.raw, heartbeat.raw, alerts.raw)]
[Amazon MSK] -> [Managed Flink (Flink SQL Jobs) | consumes telemetry.raw, alerts.raw]
[Managed Flink (Flink SQL Jobs)] -> [alerts.derived, telemetry.derived topics (on MSK)]
[alerts.derived] -> [Lambda (MSK consumer / transformer)]
[Lambda (MSK consumer / transformer)] -> [ServiceNow REST API (create incident)]
[Amazon MSK] -> [MSK Connect -> S3 (raw archive)]
[Observability] <- CloudWatch Metrics, Prometheus (JMX exporter), Grafana
[Alerting] <- CloudWatch Alarms -> SNS Topic -> Lambda -> ServiceNow (Ops alerts)
(Security): VPC, TLS, SASL/IAM, KMS
(Notes): "Use partition by device-id/site-id for ordering; Flink handles ASA-like SQL transforms & windows"


Flow 2 — Replace NiFi landing to ODS staging (paste into Lucidchart text-to-diagram)

(One-line: MSK Connect JDBC Sink writes raw device/heartbeat events directly into ODS staging tables; S3 for archival)

[Device / Event Collector] -> [On-Prem Kafka]
[On-Prem Kafka] -> [MirrorMaker / Replicator] -> [Amazon MSK (telemetry.raw, heartbeat.raw)]
[Amazon MSK] -> [MSK Connect -> JDBC Sink -> ODS Staging (RDS / Aurora / External JDBC target)]
[Amazon MSK] -> [MSK Connect -> S3 (raw partitioned archive by date/device)]
[Optional] [Amazon MSK] -> [Consumer App / Batch ETL] -> [Transform -> Final ODS / Data Warehouse]
[Monitoring] <- CloudWatch Logs + Connector Metrics
[DLQ / Errors] -> [SQS or SNS] -> [Ops Lambda / Manual Review Workflow]
(Security): VPC, TLS, IAM, Secrets Manager for DB creds, KMS
(Notes): "MVP replaces NiFi with managed JDBC sink; retain NiFi in read-only mode for complex provenance if needed."

Compact Flow
-----------------------

External:
  Event Generator (Device) -> Kafka (Topics)
  Event Generator (Device) -> Azure Event Hubs (direct ingest)

Ingest:
  Kafka (Topics) -> Kafka MirrorMaker -> Event Hubs Namespace

Transform:
  Event Hubs Namespace -> Event Hub (aggregated stream)
  Event Hub -> Azure Stream Analytics -> Event Hub (filtered/output)

Notification:
  Event Hub -> Azure Logic Apps -> ServiceNow (create ticket)

Detailed Swimlanes
------------------------
Swimlane: External
  [Event Generator - Vending Machine] --AMQP / HTTPS / Kafka--> [Event Collector / Generator App]
  [Event Generator - Vending Machine] --kafka://tcp--> [Kafka Topics]

Swimlane: Ingest
  [Kafka Topics] --> [Kafka MirrorMaker]
  [Kafka MirrorMaker] --> [Event Hubs Namespace]
  [Event Generator] --(optional direct)--> [Azure Event Hubs] 

Swimlane: Transform
  [Event Hubs Namespace] --> [Event Hub 1]
  [Event Hubs Namespace] --> [Event Hub 2]
  [Event Hub N] --> [Azure Stream Analytics]
  [Azure Stream Analytics] --> [Event Hub (aggregated/filtered output)]

Swimlane: Notification
  [Event Hub (aggregated/filtered output)] --> [Azure Logic Apps]
  [Azure Logic Apps] --> [ServiceNow (now) : Create incident/ticket]

Notes:
  - Kafka holds multiple topics (Topic1, Topic2,... TopicN)
  - Stream Analytics aggregates & filters messages before output
  - Logic App reads from Event Hub and triggers ServiceNow integration



### Option -A
Event Generator -> Kafka
Kafka -> Kafka MirrorMaker
Kafka MirrorMaker -> Azure Event Hubs Namespace
Azure Event Hubs Namespace -> Event Hub (Raw Stream)
Event Hub (Raw Stream) -> Azure Stream Analytics
Azure Stream Analytics -> Event Hub (Filtered Output)
Event Hub (Filtered Output) -> Azure Logic Apps
Azure Logic Apps -> ServiceNow (Ticket Creation)


### option-B
[Event Generator] --> [Kafka]
[Event Generator] --> [Azure Event Hubs (direct)]
[Kafka] --> [Kafka MirrorMaker]
[Kafka MirrorMaker] --> [Event Hubs Namespace]
[Event Hubs Namespace] --> [Event Hub]
[Event Hub] --> [Azure Stream Analytics]
[Azure Stream Analytics] --> [Event Hub (Output)]
[Event Hub (Output)] --> [Azure Logic Apps]
[Azure Logic Apps] --> [ServiceNow]


### Option-C
flowchart LR
    EG(Event Generator) --> K(Kafka)
    EG --> EHDirect(Azure Event Hubs - Direct Ingest)
    K --> MM(Kafka MirrorMaker)
    MM --> EHNS(Event Hubs Namespace)
    EHNS --> EHRaw(Event Hub - Raw Messages)
    EHRaw --> ASA(Azure Stream Analytics)
    ASA --> EHOut(Event Hub - Filtered Output)
    EHOut --> LA(Azure Logic Apps)
    LA --> SN(ServiceNow)



### Redshift

Short answer — **keep the MSK + Flink (stream processing) + MSK Connect → ODS pattern**, and **use Amazon Redshift as the analytics/BI sink**, **not** as a replacement for the stream processor or for the immediate ticketing/alerting path.

Why — the single crisp reasons:

* **Redshift is a data warehouse (SQL analytics, historical reporting, BI).** It’s excellent for storing aggregated/curated results and running heavy SQL reporting, dashboards, ad-hoc queries. ([Amazon Web Services, Inc.][1])
* **Redshift is not a stream-processing engine.** It doesn’t replace Flink/Apache Kafka for windowing, exactly-once stateful transformations, low-latency event detection, or driving ServiceNow incident logic in real time. For those tasks you need a stream processor (Flink/Kafka Streams) or lightweight consumers (Lambda). ([Amazon Web Services, Inc.][2])
* **Good news:** Redshift *can* ingest streaming data (direct streaming ingestion from MSK/Kinesis into materialized views), so it can receive the derived outputs of your stream jobs for near-real-time analytics and reporting — but it’s a *sink*, not a replacement for the stream compute. Use it downstream. ([AWS Documentation][3])

Concrete placement in your architecture (one clear sentence each):

* **Real-time detection & ticketing (ServiceNow):** Keep **MSK → Flink (Managed) → derived topic(s) → Lambda → ServiceNow**. Flink handles the SQL/windowing/alerts reliably and with low latency. ([Amazon Web Services, Inc.][2])
* **ODS staging (NiFi replacement MVP):** Keep **MSK Connect → JDBC sink → ODS (RDS/Aurora)** to land events as-is. This preserves the “as-is” landing requirement. ([AWS Documentation][4])
* **Analytics & reporting (where Redshift helps):** Stream the *derived* topic(s) or use MSK Connect / Firehose / Redshift streaming ingestion to populate Redshift materialized views / tables for BI, dashboards and historical SQL queries. This gives you SQL-based insights and reporting without using Azure services. ([AWS Documentation][3])

When to *use* Redshift (quick checklist):

* You need **fast, complex SQL analytics**, multi-TB scans, BI dashboards (Looker/QuickSight/Tableau). ([Amazon Web Services, Inc.][1])
* You want a **single place for curated, historical, aggregated data**, e.g., daily/weekly device health reports, SLA dashboards, long-term trend analysis.
* You want to **avoid staging everything in S3 + EMR** for ad-hoc SQL — Redshift can host curated tables and support concurrent BI users.

When *not* to use Redshift:

* For **low-latency alerting**, **windowed event detection**, **exactly-once stateful processing** (use Flink/MSK). ([Amazon Web Services, Inc.][2])
* As a direct substitute for NiFi’s provenance/flow features — use MSK Connect or keep NiFi for complex flows.

Cost/operational note (brief):

* Redshift is optimized for analytical queries and storage; streaming ingestion into Redshift is supported and can be efficient, but heavy, high-cardinality streaming workloads are typically cheaper to archive in S3 and process with streaming engines + periodic loads into Redshift. Balance retention, query patterns and cost. ([Amazon Web Services, Inc.][1])

Final one-line recommendation (the decisive answer):
**Do not replace Flink/MSK with Redshift.** Keep **MSK + Flink + MSK Connect → ODS** for the MVP and add **Redshift downstream** as your analytics/SQL reporting store (fed from derived topics or via streaming ingestion) to satisfy the SQL-insight/reporting needs and BI users. ([AWS Documentation][3])




## for now final

Nice — I’ll walk through every element in that diagram and give a crisp, practical reason why it’s there, what it does, and a short “operational tip” for each one so you can act on it. No fluff — just the useful why/how.

---

### `[Devices]`

**Why:** the source of truth — vending machines and edge devices generate telemetry, heartbeat, and alert events.
**What it does:** produces raw events (status, metrics, errors).
**Tip:** keep messages small, include device-id & timestamp, and sign/encrypt at source where possible (TLS + client certs).

---

### `[Edge/Event Collector]`

**Why:** stabilizes unreliable networks, enforces security, batches/backs up events, performs light enrichment and validation before sending to Kafka/MSK.
**What it does:** TLS termination, auth (mTLS / token), local buffering on network loss, schema validation, initial filtering, optionally compress/encrypt payloads.
**Tip:** run lightweight agent or gateway close to devices; provide durable local spool (disk) so devices can continue even with brief connectivity loss. Keep only minimal domain logic here — heavy processing should be centralized.

---

### `[Amazon MSK (multi-AZ)]`

**Why:** your durable, scalable Kafka service on AWS; keeps Kafka semantics (topics, partitions, ordering) and makes it easy to reuse existing Kafka producers/consumers. Multi-AZ for HA.
**What it does:** stores events reliably, provides partitioned parallelism, decouples producers & consumers, supports replication/retention policies.
**Tip:** choose MSK Serverless for variable workloads or Provisioned MSK for predictable high throughput. Partition by device-id or site-id to preserve ordering and scale consumers.

---

### `[Lambda Consumers (alarms/enrich)]`

**Why:** low-latency, serverless consumers for light-weight tasks (alert detection, small enrichments, bridging to REST APIs like ServiceNow). Fast to deploy and cost-efficient at low to moderate volume.
**What it does:** consumes messages, applies simple rules, writes to metrics/audit stores, or calls external APIs.
**Tip:** use MSK event-source mapping or a small Kafka client. Add DLQ (SNS/SQS) and idempotency (DynamoDB) to avoid duplicate incidents.

---

### `[Stream Processors (Flink / Kafka Streams)]`

**Why:** where stateful, windowed, exactly-once, and complex-event processing (CEP) happens — the functional equivalent of Azure Stream Analytics. Flink/Kafka Streams handle windows, joins, aggregations, late arrivals and large state.
**What it does:** executes business rules (e.g., “5 errors in 60s”), aggregates telemetry, enriches streams, and writes derived events to output topics or sinks.
**Tip:** use Managed Flink (KDA) or a Flink cluster on EKS/EMR for production stateful jobs. Persist state with RocksDB and durable checkpointing for exactly-once semantics.

---

### `[MSK Connect -> S3 (Raw Lake)]`

**Why:** durable, immutable archive of raw events for auditing, replay, reprocessing, and analytics. S3 is the canonical data lake on AWS. MSK Connect simplifies shipping Kafka topics to S3.
**What it does:** continuously writes topic data to partitioned S3 prefixes (date/device), typically in Parquet/Avro/JSON.
**Tip:** partition by date and device for common query patterns. Store schema alongside data (Glue catalog) so downstream tools can read formats easily.

---

### `[S3 (Raw Lake)] -> [NiFi / Glue -> ODS / Redshift / RDS]`

**Why:** raw archive → processing and persistent storage. NiFi (if retained) or AWS Glue performs batch/ETL transformations to land data into ODS staging tables and into analytics stores like Redshift.
**What it does:** reads raw objects, applies transformations, cleansing, column mapping, and writes to relational staging (RDS/Aurora) or analytic tables (Redshift). NiFi gives visual flows & provenance; Glue gives managed ETL / serverless Spark.
**Tip:** for MVP use MSK Connect JDBC sink for simple, immediate landing (faster). Use Glue (or NiFi if you need provenance) to do heavier transformations and to populate Redshift nightly/near-real-time.

---

### `[Stream Processors] -> [ODS / Metrics]`

**Why:** derived results and aggregations need persistence for reporting and monitoring. Stream processors write both to topics and to downstream stores.
**What it does:** emits summarized records into ODS, time-series DBs, or metrics collections (for dashboards/alerts).
**Tip:** write high-value, low-velocity aggregates to ODS/Redshift, and high-frequency metrics to a metrics store (CloudWatch, Prometheus) for dashboards.

---

### `[Lambda Consumers] -> [CloudWatch Metrics]`

**Why:** observability — Lambda writes metrics (counts, processing time, error types) so you can track system health and SLAs.
**What it does:** increments custom CloudWatch metrics and logs events for tracing.
**Tip:** emit structured logs and custom metrics with dimensions (device, site, priority) to create fine-grained alarms and dashboards.

---

### `[CloudWatch Alarms] -> [SNS Topic] -> [Lambda Transformer] -> [ServiceNow Scripted REST API]`

**Why:** operational alerting and automated incident creation in ServiceNow. CloudWatch detects anomalies/alarms, SNS routes the alarm, a Lambda normalizes the payload and calls ServiceNow’s REST API to create incidents.
**What it does:** monitors thresholds or missing-heartbeat conditions; creates IT Ops incidents, ensures retries/backoff, and records incident IDs.
**Tip:** keep the Lambda idempotent (store recent alarm hashes in DynamoDB) to avoid duplicate incidents; store credentials in Secrets Manager and implement exponential backoff for ServiceNow API limits.

---

### `[Observability Account] <- CloudWatch Metrics, Grafana, Prometheus`

**Why:** centralize monitoring across multiple accounts/environments (prod/dev) for a single pane of glass for SRE/IT Ops teams. Prometheus + Grafana give richer visualization; CloudWatch is native metrics/logs.
**What it does:** aggregates metrics and logs, visualizes trends, supports alerting & runbook links.
**Tip:** use CloudWatch Metric Streams or cross-account roles to forward metrics. Export JMX broker metrics to Prometheus (via JMX exporter) and visualize in Grafana.

---

### `(Security): VPC, TLS, IAM, KMS`

**Why:** protect data in transit and at rest, control network access, and apply least privilege.
**What it does:**

* **VPC:** isolates MSK, Connect workers, Lambdas (via VPC) from public networks.
* **TLS:** encrypts in-flight data.
* **IAM / SASL:** authenticates services (use SASL/SCRAM or IAM for client auth).
* **KMS:** encrypts data at rest (S3, RDS, MSK).
  **Tip:** enforce VPC endpoints for S3 and Secrets Manager; rotate keys/credentials; implement network ACLs and security groups with minimal exposure.

---

### `(Notes): "Heartbeat topic -> Lambda -> DynamoDB lastSeen -> Alarm if missing"`

**Why:** efficient liveness detection — instead of scanning topics you update a small key-value store with lastSeen timestamps and run threshold checks. This is cheap and fast.
**What it does:** Lambda consumes heartbeat events, updates `lastSeen[deviceId]` in DynamoDB with TTL. A scheduled checker or DynamoDB TTL/stream triggers alarms if `lastSeen` is older than allowed.
**Tip:** DynamoDB is ideal for this pattern — extremely low latency and cheap. Use DynamoDB Streams if you want event-driven checks; otherwise schedule a periodic Lambda to find stale devices and raise alarms.

---

## Quick mapping to your functional objectives

* **Real-time detection & ticketing:** MSK → Flink → derived topic → Lambda → ServiceNow (fast, stateful, exactly-once where needed).
* **As-is ODS landing:** MSK Connect (JDBC) or NiFi/Glue from S3 → RDS / ODS staging (keeps landing schema identical).
* **Audit & replay:** S3 Raw Lake (immutable archive) — always keep raw for reprocessing.
* **Observability & Ops:** CloudWatch + Prometheus + Grafana + SNS → Lambda → ServiceNow for automatic incident creation.

---

## Final one-sentence operational advice

Use MSK for durable event transport, Flink/Kafka Streams for the SQL/windowed business logic and real-time alerts, Lambda for lightweight bridging and alarms, MSK Connect/S3/Glue (or JDBC Sink) to land raw data into ODS/Redshift — and centralize observability and security with CloudWatch, Prometheus, VPC, IAM, and KMS.


